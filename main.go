// vercion 0.2 cloud
// RAM limit 100-200 MB = no DB in memory => check for doubles via select PG
// write to PG after every file red

// 1. searcher() goes to dir ./files/*, try to read files .json, .tcx, .xml, .gpx
// 2. interface it and choose right reader()
// 3. reader() makes json to structure
// 4. reader() ask DB() to check for dubles and write new date in accordance table

// this stage count 1,5 mln lines to 1,5 mln in pg in more than 15 minutes (dont know)
// possible to start goroutines to check doubles after every fire red
package main

import (
	"encoding/json"
	"fmt"
	"log"
	"os"
	"path/filepath"
	"strconv"
	"time"

	"github.com/go-pg/pg"
	"github.com/go-pg/pg/orm"
	"github.com/joho/godotenv"
)

func panicIf(err error) {
	if err != nil {
		panic(err)
	}
}

// google fit ==============================
type AutoGenerated struct {
	DataSource string      `json:"Data Source"`
	DataPoints []DataPoint `json:"Data Points"`
}

type DataPoint struct {
	FitValue           []FitVal `json:"fitValue"`
	OriginDataSourceID string   `json:"originDataSourceId"`
	EndTimeNanos       int64    `json:"endTimeNanos"`
	DataTypeName       string   `json:"dataTypeName"`
	StartTimeNanos     int64    `json:"startTimeNanos"`
	ModifiedTimeMillis int64    `json:"modifiedTimeMillis"`
	RawTimestampNanos  int      `json:"rawTimestampNanos"`
}

type FitVal struct {
	Value Valu `json:"value"`
}

type Valu struct {
	IntVal int     `json:"intVal"`
	FpVal  float64 `json:"fpVal"`
}

// ========================================
// google fit to base ===========
type GoogleFit struct { // != AutoGenerated
	DataSource         string  `json:"Data Source"`
	IntVal             int     `json:"intVal"`
	FpVal              float64 `json:"fpVal"`
	OriginDataSourceID string  `json:"originDataSourceId"`
	EndTimeNanos       int64   `json:"endTimeNanos"`
	DataTypeName       string  `json:"dataTypeName"`
	StartTimeNanos     int64   `json:"startTimeNanos"`
	ModifiedTimeMillis int64   `json:"modifiedTimeMillis"`
	RawTimestampNanos  int     `json:"rawTimestampNanos"`
}

// ==============================
type errs struct {
	Err     []string
	Doubles []string
	Old     int // qty rows data before maxdate in DB
}

var er errs
var str AutoGenerated
var row DataPoint
var exitrow GoogleFit
var exitlist []GoogleFit

func main() {
	start := time.Now() // count working time
	// read evn
	// connection DB =================================
	godotenv.Load()

	adr := os.Getenv("DBADR")
	usr := os.Getenv("DBUSER")
	pwd := os.Getenv("DBPWD")
	dbs := os.Getenv("DBTYPE")
	fdir := os.Getenv("FILEDIR")
	maxdategap, err := strconv.ParseInt(os.Getenv("MAXDATEGAP"), 10, 64) // if =-1 (minus 1) will check all dates. If =10 will find MAXDATE in DB and will add new data only after MAXDATE-10
	if err != nil {
		maxdategap = 0
	}
	checkdoubles := os.Getenv("CHECKDBLS") == "Y"
	maxrowsinmemory, _ := strconv.Atoi(os.Getenv("RAMLIM"))
	if adr == "" || usr == "" || pwd == "" || dbs == "" {
		log.Printf("ERR: ENV cant find DB specs in .env")
	}
	// connect to DB
	db := pg.Connect(&pg.Options{
		Addr:     adr,
		User:     usr,
		Password: pwd,
		Database: dbs,
	})
	defer db.Close()
	//===============================================
	// println all tables available
	//f := AutoGenerated{}
	//f.DBselect(db)
	// goes to dir
	// create if not exist ==
	err = db.CreateTable(&exitrow, &orm.CreateTableOptions{
		Temp:          false, // create temp table
		IfNotExists:   true,
		FKConstraints: true,
	})
	panicIf(err)
	// ======================
	// count total strings in DB ===
	var answerqueryint int
	_, err = db.Query(&answerqueryint, `SELECT COUNT(data_source) FROM google_fits`)
	fmt.Println("Total rows in DB before start = ", answerqueryint)
	//===============================
	// find max date in DB ===
	var answerqueryint64 int64
	_, err = db.Query(&answerqueryint64, `SELECT MAX(start_time_nanos) FROM google_fits`)
	fmt.Println("last date in DB = ", time.Unix(0, answerqueryint64))
	//===============================
	//
	//fdir := "./data/googlefit/*.json"
	durDB := time.Since(start)
	jsons, err := filepath.Glob(fdir)
	if err != nil {
		fmt.Println("ERR: DIR no json files in directory", err)
	}
	googleqty := len(jsons)
	googlered := 0
	strokqty := 0
	strokred := 0
	newrowsupload := 0

	for fl, filename := range jsons {
		valuetocheck := false
		doublerowsinfile := 0
		oldrowsinfile := 0
		f, err := os.Open(filename)
		if err != nil {
			er.Err = append(er.Err, fmt.Sprint("ERR: FILE ", fdir, " Cant open file ", filename))
			continue
		}
		//defer f.Close()
		jsonParser := json.NewDecoder(f)
		var str AutoGenerated
		if err := jsonParser.Decode(&str); err != nil {
			er.Err = append(er.Err, fmt.Sprint("ERR: JSON ", fdir, " Cant decode json. File name is ", filename))
			f.Close()
			continue
		}
		f.Close()
		googlered += 1
		strokqty += len(str.DataPoints)
		exitrow.DataSource = str.DataSource
		// read what is in srting ===
		for i, dataPoints := range str.DataPoints { // array in one file
			if len(dataPoints.FitValue) != 1 { // if some difference
				er.Err = append(er.Err, fmt.Sprint("ERR: STRUCT changed: Google Fit 'fitValue' is more than 1 in array. File name is ", filename, " error string is ", i))
				break
			}
			fitValue := dataPoints.FitValue[0]
			val := fitValue.Value //.IntVal
			// check for date. compare to MAXDATE
			if maxdategap >= 0 {
				if dataPoints.StartTimeNanos <= answerqueryint64-maxdategap {
					er.Old += 1
					oldrowsinfile += 1
					continue
				}
			}
			// ==================================
			// looking for double in DB ===
			if checkdoubles {
				var dubel GoogleFit
				_, _ = db.QueryOne(&dubel, `SELECT * FROM google_fits WHERE data_source = ? AND start_time_nanos = ?`, exitrow.DataSource, dataPoints.StartTimeNanos) // new(big.Int).SetInt64(dataPoints.StartTimeNanos)) //dataPoints.StartTimeNanos,
				if dubel.DataSource != "" {
					er.Doubles = append(er.Doubles, fmt.Sprint("DOUBLE ", i, " rows in FILE = ", filename))
					doublerowsinfile += 1
					continue
				}
			}
			//===============================
			exitrow.IntVal = val.IntVal
			exitrow.FpVal = val.FpVal
			exitrow.OriginDataSourceID = dataPoints.OriginDataSourceID
			exitrow.StartTimeNanos = dataPoints.StartTimeNanos
			exitrow.EndTimeNanos = dataPoints.EndTimeNanos
			exitrow.DataTypeName = dataPoints.DataTypeName
			exitrow.ModifiedTimeMillis = dataPoints.ModifiedTimeMillis
			exitrow.RawTimestampNanos = dataPoints.RawTimestampNanos

			exitlist = append(exitlist, exitrow)
			strokred += 1
			// check if all of rows are 0. possible new parametr on Value
			valuetocheck = valuetocheck || val.IntVal != 0 || val.FpVal != 0.0
			// =========================
			// push to pg if len>maxrowsinmemory
			if len(exitlist) >= maxrowsinmemory {
				err = db.Insert(&exitlist)
				if err != nil {
					for i := 0; i < 40; i++ {
						<-time.After(5 * time.Second)
						err = db.Insert(&exitlist)
						if err == nil {
							break
						}
					}
					if err != nil {
						panic(err)
					}
				}
				newrowsupload += len(exitlist)
				exitlist = exitlist[:0] // keep capacity. alternative is slice = nil
				fmt.Println("Sent to DB on row #", i, " continue...")
			}
			//===========
		}
		if !valuetocheck {
			googlered -= 1
			if doublerowsinfile == len(str.DataPoints) {
				er.Doubles = append(er.Doubles, fmt.Sprint("DOUBLE WHOLE FILE  = ", filename))
			} else if oldrowsinfile == len(str.DataPoints) {
				er.Doubles = append(er.Doubles, fmt.Sprint("OLD WHOLE FILE        = ", filename))
			} else if oldrowsinfile < len(str.DataPoints) {
				er.Doubles = append(er.Doubles, fmt.Sprint("OLD ", oldrowsinfile, " rows in FILE    = ", filename))
			} else {
				er.Err = append(er.Err, fmt.Sprint("ERR: STRUCT error, file may be empty or total Double, or error in struct type. File: ", filename))
			}
		}

		// push to pg
		if len(exitlist) != 0 {
			err = db.Insert(&exitlist)
			if err != nil { // try many times to reconnect DB
				for i := 0; i < 40; i++ {
					<-time.After(5 * time.Second)
					err = db.Insert(&exitlist)
					if err == nil {
						break
					}
				}
				if err != nil {
					panic(err)
				}
			}
			newrowsupload += len(exitlist)
			exitlist = exitlist[:0] // keep capacity. alternative is slice = nil

		}
		//===========
		fmt.Println("Checked all ", len(str.DataPoints), " rows from file ", filename)
		fmt.Println(fl, "/", len(jsons), " files done and uploaded to DB ", time.Since(start))
	}
	durREADandDOUBLES := time.Since(start)
	if googlered != googleqty || strokred != strokqty {
		fmt.Println("ERRORS exists!")
		for _, s := range er.Err {
			fmt.Println(s)
		}
	}
	if len(er.Doubles) != 0 {
		fmt.Println("\nDoubles: ")
		for _, s := range er.Doubles {
			fmt.Println(s)
		}
	}
	fmt.Printf("Files have correct new data %d/%d.\nUploaded %d rows from %d can read from total %d rows in files.\nOld rows qty = %d",
		googlered, googleqty, newrowsupload, strokred, strokqty, er.Old)
	fmt.Println("datebase red in ", durDB, ". Files red and Check for doubles in ", durREADandDOUBLES)
}
